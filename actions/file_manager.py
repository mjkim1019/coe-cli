# actions/file_manager.py
import os
import re
from typing import List, Optional, Dict
from .file_tree_analyzer import FileTreeAnalyzer

# ìƒˆë¡œ ì¶”ê°€: charset_normalizerë¡œ ì¸ì½”ë”© ê°ì§€
try:
    from charset_normalizer import from_bytes
except ImportError:
    from_bytes = None

class FileManager:
    def __init__(self):
        self.files = {}
        self.c_file_info = {}  # C íŒŒì¼ì˜ êµ¬ì¡° ì •ë³´ë¥¼ ì €ì¥
        self.sql_file_info = {}  # SQL íŒŒì¼ì˜ êµ¬ì¡° ì •ë³´ë¥¼ ì €ì¥
        self.tree_analyzer = FileTreeAnalyzer()  # íŒŒì¼ íŠ¸ë¦¬ ë¶„ì„ê¸°

    def add(self, file_paths):
        """íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€"""
        messages = []
        file_analyses = []
        
        for file_path in file_paths:
            if os.path.isdir(file_path):
                # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ë“¤ì„ ì¶”ê°€
                dir_messages = self.add_directory(file_path)
                messages.extend(dir_messages)
            else:
                # ê°œë³„ íŒŒì¼ ì²˜ë¦¬
                result = self.add_single_file(file_path)
                if result['message']:
                    messages.append(result['message'])
                    
                # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì €ì¥
                if result['analysis']:
                    file_analyses.append({
                        'file_path': file_path,
                        'file_type': result['file_type'],
                        'analysis': result['analysis']
                    })
        
        # íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì „ì²´ ê²°ê³¼ ë°˜í™˜
        return {
            'messages': messages,
            'analyses': file_analyses
        }

    def add_directory(self, directory_path: str, max_files: int = 50) -> List[str]:
        """ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì¶”ê°€"""
        messages = []
        
        # ë””ë ‰í† ë¦¬ ë¶„ì„
        analysis = self.tree_analyzer.analyze_directory(directory_path)
        
        if 'error' in analysis:
            messages.append(f"Error analyzing directory {directory_path}: {analysis['error']}")
            return messages
        
        # ì£¼ìš” íŒŒì¼ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ê°€
        primary_files = []
        other_files = []
        
        file_categories = analysis.get('file_categories', {})
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ íŒŒì¼ë“¤ ë¶„ë¥˜
        for category, files in file_categories.items():
            if category in ['c_files', 'header_files', 'sql_files', 'xml_files']:
                # ì£¼ìš” íŒŒì¼ë“¤
                for file_info in files:
                    primary_files.append(file_info['full_path'])
            else:
                # ê¸°íƒ€ íŒŒì¼ë“¤
                for file_info in files:
                    other_files.append(file_info['full_path'])
        
        # ì£¼ìš” íŒŒì¼ë“¤ ë¨¼ì € ì¶”ê°€
        added_count = 0
        for file_path in primary_files:
            if added_count >= max_files:
                break
            result = self.add_single_file(file_path)
            if result['message']:
                messages.append(result['message'])
                added_count += 1
        
        # ë‚¨ì€ ìš©ëŸ‰ì´ ìˆìœ¼ë©´ ê¸°íƒ€ íŒŒì¼ë“¤ë„ ì¶”ê°€
        for file_path in other_files:
            if added_count >= max_files:
                break
            result = self.add_single_file(file_path)
            if result['message']:
                messages.append(result['message'])
                added_count += 1
        
        # ë””ë ‰í† ë¦¬ ë¶„ì„ ìš”ì•½ ì¶”ê°€
        insights = analysis.get('project_insights', {})
        if insights:
            messages.append(f"\nğŸ“ Directory Analysis for {directory_path}:")
            messages.append(f"  - Project Type: {insights.get('project_type', 'unknown')}")
            messages.append(f"  - Complexity: {insights.get('complexity', 'unknown')}")
            if insights.get('characteristics'):
                messages.append(f"  - Characteristics: {', '.join(insights['characteristics'])}")
            if insights.get('tech_stack'):
                messages.append(f"  - Tech Stack: {', '.join(insights['tech_stack'])}")
        
        # ì¶”ê°€ëœ íŒŒì¼ í†µê³„
        total_files = analysis.get('total_files', 0)
        if added_count < total_files:
            messages.append(f"\nâš ï¸  Added {added_count} out of {total_files} files (limit: {max_files})")
        else:
            messages.append(f"\nâœ… Added all {added_count} files from directory")
        
        return messages

    def add_single_file(self, file_path: str) -> Dict:
        """ë‹¨ì¼ íŒŒì¼ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        result = {
            'message': '',
            'analysis': None,
            'file_type': 'unknown'
        }
        
        if os.path.exists(file_path):
            try:
                # ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ë¨¼ì € ì½ì–´ raw ë°ì´í„°ë¥¼ í™•ë³´
                with open(file_path, 'rb') as f:
                    raw_data = f.read()

                # 1) ê¸°ë³¸ì ìœ¼ë¡œ raw ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¸ì½”ë”© ê°ì§€ ì‹œë„
                detected_encoding = None
                if from_bytes:
                    try:
                        best_match = from_bytes(raw_data).best()
                        if best_match and best_match.encoding:
                            detected_encoding = best_match.encoding
                    except Exception:
                        pass

                # 2) ê°ì§€ëœ ì¸ì½”ë”©ì„ ë¨¼ì € ì‹œë„í•˜ê³ , ê·¸ ë°–ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë”© ëª©ë¡ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
                encodings_to_try = []
                if detected_encoding:
                    encodings_to_try.append(detected_encoding)
                # EUC-KR, CP949 ê°™ì€ í•œê¸€ ì¸ì½”ë”©ê³¼ UTF-8 BOM ë“±ì„ í¬í•¨í•´ ì‹œë„
                encodings_to_try += ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'shift_jis']

                content = None
                used_encoding = None
                for enc in encodings_to_try:
                    try:
                        content = raw_data.decode(enc)
                        used_encoding = enc
                        break
                    except Exception:
                        continue

                # 3) ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ëŠ” errors='replace' ì˜µì…˜ì„ ì‚¬ìš©í•´ UTF-8ë¡œ ê°•ì œ ë””ì½”ë”©
                if content is None:
                    content = raw_data.decode('utf-8', errors='replace')
                    used_encoding = 'utf-8 (fallback with replace)'

                # ì½ì€ ë‚´ìš©ê³¼ ì¸ì½”ë”© ì •ë³´ë¥¼ ì €ì¥
                self.files[file_path] = content
                line_count = len(content.splitlines())
                char_count = len(content)
                
                # .c íŒŒì¼ì¸ ê²½ìš° êµ¬ì¡° ì •ë³´ ì¶”ê°€
                if file_path.endswith('.c'):
                    result['file_type'] = 'c_file'
                    analysis = self._analyze_c_file_structure(content)
                    self.c_file_info[file_path] = analysis
                    result['analysis'] = self._enhance_c_file_analysis(content, analysis)
                    result['message'] = (
                        f"Read C file {file_path} with encoding {used_encoding} "
                        f"({line_count} lines, {char_count} chars) - Standard C structure detected"
                    )
                # .sql íŒŒì¼ì¸ ê²½ìš° êµ¬ì¡° ì •ë³´ ì¶”ê°€
                elif file_path.endswith('.sql'):
                    result['file_type'] = 'sql_file'
                    analysis = self._analyze_sql_file_structure(content)
                    self.sql_file_info[file_path] = analysis
                    result['analysis'] = analysis
                    result['message'] = (
                        f"Read SQL file {file_path} with encoding {used_encoding} "
                        f"({line_count} lines, {char_count} chars) - Oracle SQL structure detected"
                    )
                # .h íŒŒì¼ì¸ ê²½ìš° í—¤ë” êµ¬ì¡° ë¶„ì„
                elif file_path.endswith('.h'):
                    result['file_type'] = 'header_file'
                    result['analysis'] = self._analyze_header_file_structure(content, file_path)
                    result['message'] = (
                        f"Read Header file {file_path} with encoding {used_encoding} "
                        f"({line_count} lines, {char_count} chars) - Header structure detected"
                    )
                # .xml íŒŒì¼ì¸ ê²½ìš° UI êµ¬ì¡° ë¶„ì„
                elif file_path.lower().endswith('.xml'):
                    result['file_type'] = 'xml_file'
                    result['analysis'] = self._analyze_xml_file_structure(content)
                    result['message'] = (
                        f"Read XML file {file_path} with encoding {used_encoding} "
                        f"({line_count} lines, {char_count} chars) - XML UI structure detected"
                    )
                else:
                    result['message'] = (
                        f"Read {file_path} with encoding {used_encoding} "
                        f"({line_count} lines, {char_count} chars)"
                    )
                    
            except Exception as e:
                result['message'] = f"Error reading file {file_path}: {e}"
        else:
            # ìƒˆ íŒŒì¼ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”
            self.files[file_path] = ""
            result['message'] = f"Added new file (will be created on edit): {file_path}"
            
        return result

    def _analyze_c_file_structure(self, content):
        """C íŒŒì¼ì˜ í‘œì¤€ í•¨ìˆ˜ êµ¬ì¡°ë¥¼ ë¶„ì„"""
        standard_functions = {
            'a000_init_proc': 'í”„ë¡œê·¸ë¨ ì´ˆê¸°í™” í•¨ìˆ˜',
            'b000_input_validation': 'ì…ë ¥ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰',
            'b999_output_setting': 'ì¶œë ¥ ì „ë¬¸ì˜ ìˆœì„œ ì„¤ì •',
            'c000_main_proc': 'ì‹¤ì œ í”„ë¡œê·¸ë¨ì˜ ì£¼ìš” ë¡œì§ ì²˜ë¦¬',
            'c300_get_svc_info': 'ì„œë¹„ìŠ¤ ì •ë³´ ì¡°íšŒ í•¨ìˆ˜',
            'x000_mpfmoutq_proc': 'ì¶œë ¥ ì²˜ë¦¬ ìˆ˜í–‰ í•¨ìˆ˜',
            'z000_norm_exit_proc': 'í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ ì²˜ë¦¬',
            'z999_err_exit_proc': 'í”„ë¡œê·¸ë¨ ì—ëŸ¬ ì¢…ë£Œ ì²˜ë¦¬'
        }
        
        found_functions = {}
        lines = content.splitlines()
        
        for i, line in enumerate(lines):
            for func_name, description in standard_functions.items():
                if func_name in line and ('(' in line or 'void' in line or 'int' in line):
                    found_functions[func_name] = {
                        'description': description,
                        'line_number': i + 1
                    }
        
        return {
            'standard_functions': standard_functions,
            'found_functions': found_functions
        }

    def get_c_file_context(self, file_path):
        """C íŒŒì¼ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°˜í™˜"""
        if file_path in self.c_file_info:
            return self.c_file_info[file_path]
        return None

    def _analyze_sql_file_structure(self, content):
        """SQL íŒŒì¼ì˜ ì˜¤ë¼í´ êµ¬ì¡°ë¥¼ ë¶„ì„"""
        sql_features = {
            'hints': [],
            'bind_variables': [],
            'table_aliases': [],
            'outer_joins': [],
            'oracle_functions': [],
            'table_names': []
        }
        
        lines = content.splitlines()
        content_upper = content.upper()
        
        # íŒíŠ¸ ì°¾ê¸° (/*+ ... */)
        import re
        hint_pattern = r'/\*\+([^*]+)\*/'
        hints = re.findall(hint_pattern, content)
        sql_features['hints'] = [hint.strip() for hint in hints]
        
        # ë°”ì¸ë“œ ë³€ìˆ˜ ì°¾ê¸° (:variable)
        bind_pattern = r':(\w+)'
        binds = re.findall(bind_pattern, content)
        sql_features['bind_variables'] = list(set(binds))
        
        # ì•„ìš°í„° ì¡°ì¸ ì°¾ê¸° ((+))
        if '(+)' in content:
            sql_features['outer_joins'] = ['Oracle outer join syntax detected']
        
        # ì˜¤ë¼í´ í•¨ìˆ˜ ì°¾ê¸°
        oracle_functions = ['NVL', 'TO_CHAR', 'SYSDATE', 'TO_DATE', 'DECODE', 'CASE']
        for func in oracle_functions:
            if func in content_upper:
                sql_features['oracle_functions'].append(func)
        
        # ìœ íš¨ì„± ì²´í¬ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        validity_patterns = []
        if '99991231235959' in content:
            validity_patterns.append('99991231235959 (timestamp format)')
        if '99991231' in content:
            validity_patterns.append('99991231 (date format)')
        sql_features['validity_patterns'] = validity_patterns
        
        # í…Œì´ë¸” ë³„ì¹­ íŒ¨í„´ ì°¾ê¸° (ë‹¨ìˆœí•œ ë°©ì‹ìœ¼ë¡œ)
        for line in lines:
            line_stripped = line.strip()
            if 'from' in line_stripped.lower() or 'join' in line_stripped.lower():
                # ê°„ë‹¨í•œ ë³„ì¹­ íŒ¨í„´ ë§¤ì¹­
                alias_pattern = r'(\w+)\s+(\w+)(?:\s|$|,)'
                matches = re.findall(alias_pattern, line_stripped, re.IGNORECASE)
                for match in matches:
                    if len(match[1]) <= 4:  # ì§§ì€ ë³„ì¹­ìœ¼ë¡œ ê°€ì •
                        sql_features['table_aliases'].append(f"{match[0]} as {match[1]}")
        
        return sql_features

    def get_sql_file_context(self, file_path):
        """SQL íŒŒì¼ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°˜í™˜"""
        if file_path in self.sql_file_info:
            return self.sql_file_info[file_path]
        return None
    
    def analyze_directory_structure(self, directory_path: str) -> dict:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜"""
        return self.tree_analyzer.analyze_directory(directory_path)
    
    def suggest_files_for_query(self, directory_path: str, user_query: str) -> List[str]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ê¸°ë°˜í•˜ì—¬ ê´€ë ¨ íŒŒì¼ë“¤ì„ ì¶”ì²œ"""
        return self.tree_analyzer.suggest_files_for_context(directory_path, user_query)
    
    def find_files_by_pattern(self, directory_path: str, pattern: str) -> List[str]:
        """íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ë“¤ì„ ì°¾ê¸°"""
        return self.tree_analyzer.find_files_by_pattern(directory_path, pattern)
    
    def _enhance_c_file_analysis(self, content: str, basic_analysis: Dict) -> Dict:
        """C íŒŒì¼ì— ëŒ€í•œ í–¥ìƒëœ ë¶„ì„"""
        enhanced = basic_analysis.copy()
        
        # í—¤ë” íŒŒì¼ include ë¶„ì„ (ì„¹ì…˜ë³„ë¡œ ë¶„ë¥˜)
        includes = self._categorize_includes(content)
        enhanced['includes'] = includes
        
        # IO êµ¬ì¡°ì²´ íŒ¨í„´ ì°¾ê¸°
        io_patterns = {
            'input_structs': [],
            'output_structs': [],
            'context_structs': []
        }
        
        # Input/Output êµ¬ì¡°ì²´ ì°¾ê¸° (pio_*_in.h íŒ¨í„´)
        all_includes = includes.get('io_formatter', []) + includes.get('static_library', []) + includes.get('other', [])
        for include in all_includes:
            if 'pio_' in include and '_in.h' in include:
                io_patterns['input_structs'].append(include)
            elif 'pio_' in include and '_out' in include:
                io_patterns['output_structs'].append(include)
        
        # Context êµ¬ì¡°ì²´ ì°¾ê¸°
        ctx_pattern = r'typedef\s+struct\s+(\w*ctx\w*)\s+(\w+);'
        for match in re.finditer(ctx_pattern, content, re.IGNORECASE):
            io_patterns['context_structs'].append(match.group(2))
        
        enhanced['io_structures'] = io_patterns
        
        # DBIO íŒ¨í„´ ì°¾ê¸°
        enhanced['dbio_includes'] = includes.get('dbio_library', [])
        
        # Static Library í—¤ë”ë“¤ (ì¤‘ìš”í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
        enhanced['static_library_includes'] = includes.get('static_library', [])
        
        return enhanced
    
    def _categorize_includes(self, content: str) -> Dict[str, List[str]]:
        """í—¤ë” íŒŒì¼ë“¤ì„ ì„¹ì…˜ë³„ë¡œ ë¶„ë¥˜"""
        categories = {
            'system': [],           # ì‹œìŠ¤í…œ í—¤ë” (<pfm*.h> ë“±)
            'io_formatter': [],     # IO Formatter ì„¹ì…˜
            'static_library': [],   # Static Library ì„¹ì…˜ (ì¤‘ìš”!)
            'dbio_library': [],     # DBIO Library ì„¹ì…˜
            'other': []
        }
        
        lines = content.split('\n')
        current_section = 'other'
        
        for line in lines:
            line = line.strip()
            
            # ì„¹ì…˜ ê°ì§€
            if '--- IO Formatter   ---' in line or 'IO Formatter' in line:
                current_section = 'io_formatter'
                continue
            elif '--- Static Library ---' in line or 'Static Library' in line:
                current_section = 'static_library'
                continue
            elif '--- DBIO Library    ---' in line or 'DBIO Library' in line:
                current_section = 'dbio_library'
                continue
            
            # include ë¬¸ íŒŒì‹±
            include_match = re.match(r'#include\s*[<"](.*?)[>"]', line)
            if include_match:
                include_file = include_match.group(1)
                
                # ì‹œìŠ¤í…œ í—¤ë” ë¶„ë¥˜
                if include_file.startswith('pfm') or include_file.startswith('<'):
                    categories['system'].append(include_file)
                else:
                    categories[current_section].append(include_file)
        
        return categories
    
    def _analyze_header_file_structure(self, content: str, file_path: str = '') -> Dict:
        """í—¤ë” íŒŒì¼ êµ¬ì¡° ë¶„ì„"""
        analysis = {
            'type': 'unknown',
            'structures': [],
            'struct_details': {},
            'defines': [],
            'functions': []
        }
        
        # êµ¬ì¡°ì²´ ì •ì˜ì™€ ë‚´ìš© ì°¾ê¸°
        struct_pattern = r'struct\s+(\w+)\s*\{([^}]*)\}'
        for match in re.finditer(struct_pattern, content, re.DOTALL):
            struct_name = match.group(1)
            struct_body = match.group(2)
            analysis['structures'].append(struct_name)
            analysis['struct_details'][struct_name] = self._parse_struct_fields(struct_body)
        
        # typedef êµ¬ì¡°ì²´ ì°¾ê¸°
        typedef_pattern = r'typedef\s+struct\s+\w*\s*\{([^}]*)\}\s*(\w+);'
        for match in re.finditer(typedef_pattern, content, re.DOTALL):
            struct_body = match.group(1)
            struct_name = match.group(2)
            analysis['structures'].append(struct_name)
            analysis['struct_details'][struct_name] = self._parse_struct_fields(struct_body)
        
        # #define ì°¾ê¸° (ê¸¸ì´ ì •ì˜ë„ í¬í•¨)
        define_pattern = r'#define\s+(\w+)(?:\s+(.+?))?(?:\n|$)'
        for match in re.finditer(define_pattern, content):
            define_name = match.group(1)
            define_value = match.group(2).strip() if match.group(2) else ''
            analysis['defines'].append({
                'name': define_name,
                'value': define_value
            })
        
        # í—¤ë” íŒŒì¼ íƒ€ì… ê²°ì • (íŒŒì¼ëª…ë„ í™•ì¸)
        if ('pio_' in content and ('_in' in content or '_out' in content)) or ('pio_' in file_path and ('_in.h' in file_path or '_out' in file_path)):
            analysis['type'] = 'io_structure'
        elif 'pdb_' in content or 'pdb_' in file_path:
            analysis['type'] = 'dbio_structure'
        elif any(func in content for func in ['void', 'int', 'long', 'char']):
            analysis['type'] = 'function_declarations'
        
        return analysis
    
    def _parse_struct_fields(self, struct_body: str) -> List[Dict]:
        """êµ¬ì¡°ì²´ í•„ë“œë“¤ì„ íŒŒì‹±"""
        fields = []
        lines = struct_body.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('/*') or line.startswith('*') or line.startswith('//'):
                continue
            
            # í•„ë“œ íŒ¨í„´: type field_name[size]; /* comment */
            field_pattern = r'(\w+(?:\s*\*)?)\s+(\w+)(?:\s*\[([^\]]+)\])?\s*;(?:\s*/\*\s*(.+?)\s*\*/)?'
            match = re.match(field_pattern, line)
            
            if match:
                field_type = match.group(1).strip()
                field_name = match.group(2)
                field_size = match.group(3) if match.group(3) else None
                field_comment = match.group(4) if match.group(4) else ''
                
                fields.append({
                    'type': field_type,
                    'name': field_name,
                    'size': field_size,
                    'comment': field_comment
                })
        
        return fields
    
    def get_struct_info(self, file_path: str) -> Optional[Dict]:
        """íŠ¹ì • í—¤ë” íŒŒì¼ì˜ êµ¬ì¡°ì²´ ì •ë³´ ë°˜í™˜"""
        if file_path not in self.files:
            return None
        
        content = self.files[file_path]
        return self._analyze_header_file_structure(content, file_path)
    
    def _analyze_xml_file_structure(self, content: str) -> Dict:
        """XML íŒŒì¼ êµ¬ì¡° ë¶„ì„ (UI í™”ë©´)"""
        analysis = {
            'form_id': '',
            'datasets': [],
            'ui_components': [],
            'functions': []
        }
        
        # Form ID ì°¾ê¸°
        form_id_pattern = r'FormID\(ëª…\)\s*:\s*(\w+)'
        match = re.search(form_id_pattern, content)
        if match:
            analysis['form_id'] = match.group(1)
        
        # Dataset ì°¾ê¸°
        dataset_pattern = r'(d[sl]_\w+|DS_\w+)'
        datasets = set(re.findall(dataset_pattern, content, re.IGNORECASE))
        analysis['datasets'] = list(datasets)
        
        # UI ì»´í¬ë„ŒíŠ¸ ì°¾ê¸°
        ui_components = []
        if 'gridView' in content:
            ui_components.append('gridView')
        if 'textbox' in content:
            ui_components.append('textbox')
        if 'input' in content:
            ui_components.append('input')
        if 'trigger' in content:
            ui_components.append('button')
        analysis['ui_components'] = ui_components
        
        # JavaScript í•¨ìˆ˜ ì°¾ê¸°
        js_func_pattern = r'scwin\.(\w+)\s*='
        functions = set(re.findall(js_func_pattern, content))
        analysis['functions'] = list(functions)
        
        return analysis
